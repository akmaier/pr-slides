\def\PS{{\mathop{\mathsf{PS}}}}

\section{Model Assessment}

\subsection{No Free Lunch}

\begin{frame}
  \frametitle{No Free Lunch}

  \begin{itemize}
    \item In the past lectures, we have come across many learning algorithms and classification techniques.
    \item They have properties such as
      \begin{itemize} 
        \item low computational complexity
        \item incorporation of prior knowledge
        \item linearity\,/\,non-linearity
        \item optimality with respect to certain cost functions, etc.
      \end{itemize}
   \item Some compute smooth decision boundaries, some compute rather non-smooth decision boundaries.
  \end{itemize}
  \pspread

  We really have to ask:
  \begin{center}
    \structure{Are there any reasons to favor one algorithm over another?}
  \end{center}
\end{frame}


\begin{frame}
  \frametitle{No Free Lunch \cont}

  \begin{theorem}
    Given a cost function $f \in {\mathcal{F}}$, an algorithm $A$ and costs $c_m$ for a specific sample that is iterated on $m$ times.\\[.3cm]
    The performance of an algorithm is the conditional probability $P(c_m | f, m, A)$.\\[.5cm] \pause
    The \structure{No Free Lunch Theorem} states that for any two algorithms $A_1$ and $A_2$:
    \begin{displaymath}
      \sum_f P(c_m | f, m, A_1) = \sum_f P(c_m | f, m, A_2)
    \end{displaymath}
  \end{theorem}
\end{frame}


\begin{frame}
  \frametitle{No Free Lunch \cont}

  \structure{Consequences for classification methods:}

  \begin{itemize}
    \item If no prior assumptions about the problem are made, there is \textbf{NO} overall superior or inferior classification method! \pause
    \item We should be skeptical regarding studies that demonstrate the overall superiority of a particular method.\\[0.5cm] \pause
    \item We have to focus on the aspects that matter most for the classification problem:
      \begin{itemize}
        \item prior information
        \item data distribution
        \item amount of training data
        \item cost functions
      \end{itemize}
  \end{itemize}
\end{frame}



\subsection{Off-Training Set Error}

\begin{frame}
  \frametitle{Off-Training Set Error}

  \structure{Off-training set error:}
  
  \begin{itemize}
    \item Specifies the error on samples that are not contained within the training set.
    \item For large training data sets, the off-training set is necessarily small.
    \item Used to compare general classification performance of algorithms.
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Off-Training Set Error \cont}
  
  \begin{itemize}
    \item Consider a two-class problem with training data set ${\mathcal{D}}$ consisting of patterns $\vec x_i$ and labels $y_i = \pm 1$. \pause
    \item $y_i$ is generated by an unknown target function: $F(\vec x_i) = y_i$. \pause
    \item The expected off-training set classification error for the $k$-th learning algorithm is:
      \begin{displaymath}
        E_k \{e | F, n\} = \sum_{\vec x \notin {\mathcal{D}}} p(\vec x) \left[ 1 - \delta(F(\vec x), h(\vec x)) \right] p_k(h(\vec x) | {\mathcal{D}})
      \end{displaymath}
      where $e$ is the error and $h(\vec x)$ the hypothesis on the data.
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Off-Training Set Error \cont}

  \begin{figure}
    \centering
    \resizebox{0.7\linewidth}{!}{
      \input{\texfigdir/off_training_performance.pstex_t}
    }
    \vspace*{0.5cm}
    \caption{\footnotesize Each square represents all possible classification problems. $+$/$-$ indicates better/worse generalization than the average (adapted from Duda, Hart).}
  \end{figure}
\end{frame}


\subsection{Bias and Variance}

\begin{frame}
  \frametitle{Bias and Variance}

  \begin{itemize}
    \item The \emph{No Free Lunch Theorem} states that there is no general best classifier. \pause
    \item But we have to assess the quality of a learning algorithm in terms of the \structure{alignment} to the classification problem. \pause
    \item This can be achieved using the \structure{bias-variance} relation.
  \end{itemize}
  \pspread

  \structure{Bias:}

  \begin{itemize}
    \item The bias measures the accuracy or quality of the match: \\
      \structure{high bias means poor match}.
  \end{itemize}
  \pspread

  \structure{Variance:}
  
  \begin{itemize}
    \item The variance measures the precision of specificity for the match: \\
      \structure{high variance implies a weak match}.
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Bias and Variance for Regression}

  The bias-variance relation is very demonstrative in the context of regression:
  
  \begin{itemize}
    \item Let $g(\vec x ; {\mathcal{D}})$ be the regression function. \pause
    \item The mean-square deviation from the true function $F(\vec x)$ is:
      \begin{eqnarray*}
         \lefteqn{E_{\mathcal{D}} \left\{ \big(g(\vec x; {\mathcal{D}}) - F(\vec x)\big)^2 \right\}} \\
         & & = \underbrace{E_{\mathcal{D}} \Big\{g(\vec x ; {\mathcal{D}}) - F(\vec x) \Big\}^2}_{\mbox{\small (\structure{bias})}^2} + 
               \underbrace{E_{\mathcal{D}} \left\{ \Big(g(\vec x; {\mathcal{D}}) - E_{\mathcal{D}} \left\{ g(\vec x; {\mathcal{D}} ) \right\} \Big)^2 \right\}}_{\mbox{\small \structure{variance}}}
      \end{eqnarray*}
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Bias and Variance for Regression \cont}

  \structure{Bias-Variance Trade-Off:}

  \begin{itemize}
    \item Methods with \structure{high flexibility} to adapt to the training data
      \begin{itemize}
        \item generally have low bias
        \item but yield high variance. \pause
      \end{itemize}
    \item Methods with \structure{few parameters} and less degrees of freedom 
      \begin{itemize}
        \item tend to have a high bias, as they may not fit the data well. 
        \item However, this does not change a lot between different data sets, so \\
          these methods generally have low variance. \pause
      \end{itemize}
    \item Unfortunately, we can virtually never get both zero bias and zero variance! \pause
    \item We need to have as \structure{much prior information} about the problem as possible to reduce both values.
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Bias and Variance for Regression \cont}

  \begin{center}
    \resizebox{0.6\linewidth}{!}{
      \input{\texfigdir/bias_variance_regression.pstex_t}
    }
  \end{center}
\end{frame}


\begin{frame}
  \frametitle{Bias and Variance for Classification}

  Assuming a \structure{two-class classification problem}:

  \begin{itemize}
    \item In a two-class problem, the target function changes to:
      \begin{displaymath}
        F(\vec x) = p(y = 1 | \vec x) = 1 - p(y = -1 | \vec x)
      \end{displaymath}
      \pause
    \item We cannot compare $g(\vec x ; {\mathcal{D}})$ and $F(\vec x)$ based on the mean-square error as in regression. \\[.3cm] \pause
    \item For simplicity, let us assume \structure{identical priors}: $p_1 = p_2 = 0.5$
      \begin{itemize}
        \item The Bayes discriminant $y_B$ has the threshold $0.5$.
        \item The Bayes decision boundary is the set of points for which $F(\vec{x}) = 0.5$.
      \end{itemize}
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Bias and Variance for Classification \cont}

  \structure{Boundary error}
  
  \begin{itemize}
    \item $p(g(\vec{x}; \mathcal{D}))$ is the pdf of obtaining a particular estimate of the discriminant given $\mathcal{D}$.
    \item Because of random variations in the training set, the boundary error will depend upon $p(g(\vec{x}; \mathcal{D}))$.
  \end{itemize}

  \begin{displaymath}
    p(g(\vec x; {\mathcal{D}}) \neq y_B) =
     \left\{
       \begin{array}{l l}
         \displaystyle{ \int_{0.5}^\infty    p(g(\vec{x}; \mathcal{D})) \, \mathsf{d}g } & \mbox{if~} F(\vec{x}) <   0.5 \\[.5cm]
         \displaystyle{ \int_{-\infty}^{0.5} p(g(\vec{x}; \mathcal{D})) \, \mathsf{d}g } & \mbox{if~} F(\vec{x}) \le 0.5 \\        
       \end{array}
     \right.
  \end{displaymath}
\end{frame}


\begin{frame}
  \frametitle{Bias and Variance for Classification \cont}

  \begin{itemize}
    \item Convenient assumption that \structure{$p(g(\vec{x}; \mathcal{D}))$ is a Gaussian}:
      \begin{eqnarray*}
        \lefteqn{p(g(\vec x; {\mathcal{D}}) \neq y_B) =} \\
        & & \Phi \Bigg[ 
              \underbrace{\mathsf{sgn}\left(F(\vec x) - \frac{1}{2}\right) \cdot \left(E_{\mathcal{D}} \{ g(\vec x ; {\mathcal{D}}) \} - \frac{1}{2}\right) }_{\mbox{\small \structure{boundary bias}}}
              \cdot \underbrace{\mathsf{var\Big(g(\vec x ; {\mathcal{D}})\Big)^{-1/2}}}_{\mbox{\small \structure{variance}}} 
            \Bigg]
      \end{eqnarray*}
      where $\Phi$ is a \structure{nonlinear function}: \\ \pause
      \begin{displaymath}
        \Phi(t) = \frac{1}{\sqrt{2\pi}} \int_t^\infty e^{-\frac{1}{2} u^2} \, \mathsf{d}u
      \end{displaymath}
    \item $p(g(\vec x; {\mathcal{D}}) \neq y_B)$ represents the incorrect estimation of the Bayes boundary.
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Bias and Variance for Classification \cont}

  \structure{Conclusions:}

  \begin{itemize}
    \item In \structure{regression} the bias-variance relation is additive in (bias)$^2$ and variance. \pause
    \item For \structure{classification} the relation is multiplicative and nonlinear. \pause
    \item In classification the sign of the boundary bias affects the role of the variance in the error. \pause
    \item Therefore, low variance is generally important for accurate classification.
  \end{itemize}
  \pspread

  \structure{Variance generally dominates bias in classification!}
\end{frame}


\begin{frame}
  \frametitle{Bias and Variance for Classification \cont}

  \def\covA{\mat{\Sigma}_i = \left(\begin{matrix} \sigma^2_{i1} & \sigma_{i12} \\ \sigma_{i12} & \sigma^2_{i2} \end{matrix}\right)}
  \def\covB{\mat{\Sigma}_i = \left(\begin{matrix} \sigma^2_{i1} & 0            \\ 0            & \sigma^2_{i2} \end{matrix}\right)}
  \def\covC{\mat{\Sigma}_i = \left(\begin{matrix} 1             & 0            \\ 0            & 1             \end{matrix}\right)}

  \begin{center}  
    \resizebox{\linewidth}{!}{
      \input{\texfigdir/bias_variance_classification.pstex_t}
    }
  \end{center}
  \spread

  \tiny{Adapted from Duda, Hart}
\end{frame}

\input{nextTime.tex}

\subsection{Estimating Bias and Variance}

\begin{frame}
  \frametitle{Resampling for Estimating Statistics}

  \structure{Problem:}
  
  \begin{itemize}
    \item Determine the bias and variance for some learning algorithm applied to a new problem with unknown distributions.
  \end{itemize}
  \pspread

  From what we have seen so far, bias and variance change with varying samples.
  \pspread

  \structure{Resampling techniques} can be used to yield more informative estimates of a general statistics.
\end{frame}


\begin{frame}
  \frametitle{Resampling for Estimating Statistics \cont}

  \structure{Formally:}
  
  \begin{itemize}
    \item Suppose we want to estimate a parameter $\theta$ that depends on a random sample set $X = (x_1, \ldots, x_n)$.
    \item Assume we have an estimator $\phi_n(X)$ of $\theta$ but do not know its distribution.
  \end{itemize}
  \pspread

  \begin{itemize}
    \item Resampling methods try to estimate the bias and variance of $\phi_n(X)$ using subsamples from $X$.
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Jackknife}
  
  Let $\PS_i(X)$ be the $i$-th pseudovalue of $\phi_n(X)$:

  \begin{eqnarray*}
    \PS_i(X) 
      &=& n \phi_n(X) - (n-1) \phi_{n-1}(X_{(i)}) \\
      &=& \phi_n(X) - \underbrace{(n-1)( \phi_{n-1}(X_{(i)}) - \phi_n(X))}_{\mathsf{bias}_{\mathsf{jack}}}
  \end{eqnarray*}
  where $X_{(i)} = (x_1, \ldots, x_{i-1}, x_{i+1}, \ldots, x_n)$ is the set without the $i$-th element.
  \pspread

  \structure{Notes:}
  
  \begin{itemize}
    \item $\PS_i(X)$ can be interpreted as a bias-corrected version of $\phi_n(X)$: \pause
    \item The bias trend is assumed to be in the estimators from $\phi_{n-1}(X_{(i)})$ to $\phi_n(X)$.
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Jackknife \cont}

  \structure{Jackknife Principle:}

  \begin{itemize}
    \item The pseudovalues $\PS_i(X)$ are treated as independent random variables with mean $\theta$. \pause
    \item Using the central limit theorem, the ML estimators for the mean $\mu_{\PS}$ and variance $\sigma_{\PS}^2$ of the pseudovalues are:
      \begin{eqnarray*}
        \mu_{\PS}      &=& \frac{1}{n} \sum_{i=1}^n \PS_i(X) \\
        \sigma_{\PS}^2 &=& \frac{1}{n-1} \sum_{i=1}^n (\PS_i(X) - \mu_{\PS})^2
      \end{eqnarray*}
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Jackknife \cont}

  \begin{example}
    \small
    Estimator for the sample mean: $\phi_n(X) = \frac{1}{n} \sum_{i=1}^n x_i = \overline{X}$ \\[.3cm] \pause 
    
    Pseudovalues of $\phi_n(X)$:
    \begin{displaymath}
      \PS_i(X) = n\overline{X} - (n-1) \overline{X_{(i)}} = x_i
    \end{displaymath}
    \pause
    
    Jackknife estimates:
    \begin{eqnarray*}
      \mu_{\PS}      &=& \frac{1}{n} \sum_{i=1}^n \PS_i(X) = \overline{X} \\
      \sigma_{\PS}^2 &=& \frac{1}{n-1} \sum_{i=1}^n (x_i - \overline{X})^2
    \end{eqnarray*}
  \end{example}
\end{frame}


\begin{frame}
  \frametitle{Jackknife \cont}

  \begin{example}
    \small
    Estimator for sample variance: $\phi_n(X) = \frac{1}{n} \sum_{i=1}^n (x_i - \overline{X})^2$ \\[.3cm] \pause
    
    Pseudovalues of $\phi_n(X)$:
    {\footnotesize
    \begin{displaymath}
      \PS_i(X) = \frac{n}{n-1} (x_i - \overline{X})^2
    \end{displaymath}
    }
    \pause

    Which implies that:
    {\footnotesize
    \begin{displaymath}
      \mu_{\PS} = \frac{1}{n} \sum_{i=1}^n \PS_i(X) = \frac{1}{n-1} \sum_{i=1}^n (x_i - \overline{X})^2
    \end{displaymath}
    }
    \pause

    {\color{gr3} Interestingly:}
    
    \begin{itemize}
      \item[\color{gr3} $\bullet$] $E\{\phi_n(X)\} = \frac{n-1}{n}\sigma^2$ whereas $E\{ \mu_{\PS} \} = \sigma^2$
      \item[\color{gr3} $\bullet$] $\mu_{\PS}$ is a bias-corrected version of $\phi_n(X)$
    \end{itemize}
  \end{example}
\end{frame}


\begin{frame}
  \frametitle{Bootstrap}

  \structure{Literary Sidenote:}\\
	The term bootstrap comes from the story: \textit{The adventures of Baron M{\"u}nchhausen}.
  \pspread

  \begin{itemize}
    \item A \textit{bootstrap} data set is created by randomly selecting $n$ points from the sample set with replacement. \pause
    \item In \textit{bootstrap estimation} this selection process is independently repeated $B$ times. \pause
    \item The $B$ bootstrap data sets are treated as independent sets.
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Bootstrap \cont}
  
  The bootstrap estimate of a statistic $\theta$ and its variance are the mean of the $B$ estimates $\hat{\theta}^B$ and its variance:
  \begin{eqnarray*}
    \mu_{\mathsf{BS}}      &=& \frac{1}{B} \sum_{i=1}^B \hat{\theta}^B_i \\
    \sigma^2_{\mathsf{BS}} &=& \frac{1}{B-1} \sum_{i=1}^B \left( \hat{\theta}^B_i - \mu_{\mathsf{BS}} \right)^2
  \end{eqnarray*}
  \pspread

  The bias is the difference between the bootstrap estimate and the estimator $\phi_n(X)$:
  \begin{displaymath}
    \mathsf{bias}_{\mathsf{BS}} = \mu_{\mathsf{BS}} - \phi_n(X)
  \end{displaymath}
\end{frame}


\begin{frame}
  \frametitle{Bootstrap \cont}

  \structure{Properties of the bootstrap estimate:}

  \begin{itemize}
    \item Bootstrapping does not change the prior of the data (choose with replacement). \pause
    \item The larger the number $B$, the more will the bootstrap estimate tend towards the true statistic $\theta$. \pause
    \item In contrast, the jackknife estimator requires exactly $n$ repetitions: 
      \begin{itemize}
        \item less than $n$ repetitions yield poorer estimates
        \item more than $n$ repetitions merely duplicate information already provided
      \end{itemize}
  \end{itemize}
\end{frame}


\subsection{Estimating and Comparing Classifiers}

\begin{frame}
  \frametitle{Estimating and Comparing Classifiers}

  Two reasons why we want to know the generalization rate of a classifier on a given problem:

  \begin{enumerate}
    \item to see if the classifier performs well enough to be useful
    \item to compare its performance with a competing design
  \end{enumerate}
\end{frame}


\begin{frame}
  \frametitle{Cross-Validation}
  
  \begin{itemize}
    \item In \structure{cross-validation}, the training samples are split into two disjoint parts:\\[.1cm]
      \begin{itemize}
        \item The first set is the training set used for the traditional training.
        \item The second set is the test set used to estimate the classification error.
        \item In a second step, both sets are swapped. 
        \item By that, the classification error can be estimated on the complete data set.
        \item Yet training and test set are always disjoint. \\[.3cm] \pause
      \end{itemize}
    \item An \structure{$m$-fold cross-validation} splits the data into $m$ disjoint sets of size $n/m$:\\[.1cm]
      \begin{itemize}
        \item 1 set is used as test set.
        \item The other $m-1$ sets are used for training. 
        \item Each set is used once for testing. \\[.3cm] \pause
      \end{itemize}
    \item In the \structure{extreme case of $m=n$}, we have  a jackknife estimate of the classification accuracy.
    %\item If we train on a bootstrap set and test on another bootstrap set of the data, we get a bootstrap estimate for the classification accuracy.
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Cross-Validation \cont}

  The classifier is trained until a minimum validation error is reached \\
  (good generalization vs.\ overfitting):
  
  \begin{figure}
    \centering
    \resizebox{.6\linewidth}{!}{
      \input{\texfigdir/cross_validation_error.pstex_t}
    }
    \caption{\footnotesize{The validation error plotted against the amount of training data (adapted from Duda, Hart).}}
  \end{figure}
\end{frame}


\subsection{Lessons Learned}

\begin{frame}
  \frametitle{Lessons Learned}
  
  \begin{itemize}
    \item There is no such thing as a free lunch! \\[.3cm]
    \item Bias-variance trade-off \\[.3cm]
    \item Jackknife \\[.3cm]
    \item Bootstrap \\[.3cm]
    \item Cross-Validation
  \end{itemize}
\end{frame}

\input{nextTime.tex}

\subsection{Further Readings}

\begin{frame}
  \frametitle{Further Readings}
  
  Examples and various content have been taken from: \\[.3cm]
  
  \begin{itemize}
    \item Richard O.\ Duda, Peter E.\ Hart, David G.\ Stork: Pattern Classification, 2nd Edition, John Wiley \& Sons, New York, 2000. \\[.3cm]
    \item S.\ Sawyer: Resampling Data: Using a Statistical Jackknife, Washington University, 2005.
  \end{itemize}
  \spread

  \structure{Further reading:}

  \begin{itemize}
    \item T. Hastie, R. Tibshirani, J. Friedman: The Elements of Statistical Learning, 2nd Edition, Springer, 2009.
  \end{itemize}
\end{frame}


\subsection{Comprehensive Questions}

\begin{frame}
  \frametitle{Comprehensive Questions}
  
  \begin{itemize}
    \item What is the meaning of the terms bias and variance? \\[.5cm]
    \item What is the difference in bias-variance trade-off between regression and classification? \\[.5cm]
    \item How do you estimate the bias and variance of a method? \\[.5cm]
    \item What is cross-validation and how can it be used to train a classifier?
  \end{itemize}
\end{frame}

